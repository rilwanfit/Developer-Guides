<!----- Conversion time: 10.888 seconds.


Using this Markdown file:

1. Cut and paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β14
* Sun Feb 10 2019 09:42:59 GMT-0800 (PST)
* Source doc: https://docs.google.com/open?id=1Jo4REa4IJzoPX1UBWAXzLl_HWGo2UJZyg4sZC2IozfQ
* This document has images: check for >>>>>  gd2md-html alert:  inline image link in generated source and store images to your server.

WARNING:
You have 4 H1 headings. You may want to use the "H1 -> H2" option to demote all headings by one level.

----->


<p style="color: red; font-weight: bold">>>>>>  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 21.</p>
<ul style="color: red; font-weight: bold"><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with >>>>>  gd2md-html alert:  for specific instances that need correction.</ul>

<p style="color: red; font-weight: bold">Links to alert messages:</p><a href="#gdcalert1">alert1</a>
<a href="#gdcalert2">alert2</a>
<a href="#gdcalert3">alert3</a>
<a href="#gdcalert4">alert4</a>
<a href="#gdcalert5">alert5</a>
<a href="#gdcalert6">alert6</a>
<a href="#gdcalert7">alert7</a>
<a href="#gdcalert8">alert8</a>
<a href="#gdcalert9">alert9</a>
<a href="#gdcalert10">alert10</a>
<a href="#gdcalert11">alert11</a>
<a href="#gdcalert12">alert12</a>
<a href="#gdcalert13">alert13</a>
<a href="#gdcalert14">alert14</a>
<a href="#gdcalert15">alert15</a>
<a href="#gdcalert16">alert16</a>
<a href="#gdcalert17">alert17</a>
<a href="#gdcalert18">alert18</a>
<a href="#gdcalert19">alert19</a>
<a href="#gdcalert20">alert20</a>
<a href="#gdcalert21">alert21</a>

<p style="color: red; font-weight: bold">>>>>> PLEASE check and correct alert issues and delete this message and the inline alerts.<hr></p>



## What is cloud?

A computer that is located somewhere else, that you are utilizing in some fashion — such as: Compute power, storage, web hosting, database management


## Common service of cloud computing?

Computing power, storage, databases, networking, analytics, developer tools, virtualization, security


## Primary benefits of using cloud computing platforms?

High availability, fault tolerant, scalability, elasticity


## Cloud terminology


### High availability

Constant and always-available access to your data or services that are stored in the cloud.


### Fault tolerance

The ability for a cloud system to have automated workarounds for when a part of the system fails.


### Scalability

The ability for a cloud system to quickly increase in size based on demand.


### Elasticity 

The ability for a cloud system to both quickly increase or decrease in size based on demand.


## What is AWS?

A cloud services provider (Iaas)- Infrastructure as a Service


## What is VPC?

Virtual Private Cloud. It is your private section of AWS, that you can place AWS resources, and allow or deny access to them.


## What is EC2

Think of a "basic" or server computer. A virtual computer that you can use for whatever you like.


## EC2 common uses?

Web hosting, encoding/transcoding


## What is instance?

A single iteration of an EC2 server.


## What is RDS?

One of AWS's supported database platforms.


## RDS common uses?

Storing customer account info, storing an inventory catalog


## What is S3?

An "unlimited" storage bucket for all your files or data.


## S3 common uses?

Mass storage, long-term storage


## AWS availability zone?

A group of AWS data centers located in a geographical region. Availability Zones provide redundancy within an AWS region.


## AWS Region

Groupings of Availability Zones and data centers that are located around the globe. Allows users to store data or use AWS resources that are physically located close to them (reduces latency)


# CompTIA Cloud Essentials



*   cloud computing from a business perspective.
*   cloud computing from a technical perspective.
*   the major differences between public and private clouds
*   techniques for cloud computing deployment
*   cloud computing risks and challenges
*   the impact of cloud computing on application development.
*   the steps an organization must take to successfully adopt cloud computing.


### Attributes of cloud computing


#### On-demand service

This is access to service anytime, anywhere.


#### Measured service

Usage fees are charged based on the amount of resources actually used 


#### Elasticity

The ability for a cloud system to both quickly increase or decrease in size based on demand.


#### Broad network access

Access to cloud services from any type of computing device.


#### Resource pooling

The cloud services provider invests in the proper hardware, configuration, and maintenance.


### Cloud computing and virtualization


#### Virtualization

This is the ability to create virtual computer hardware platforms on a single device rather than using physical hardware.


#### Hypervisor


#### Benefits of virtualization


### Early examples of cloud computing



*   1960

    Mainframe timesharing

*   Highly expensive
*   Hard to access by everybody
*   Small amount of time allowed to the companies to run the service and pay for the time they access it.
*   Similar to resource pooling model in nowadays cloud computing.
*   Early 1990

    Application service providers, Microsoft Hotmail

*   Early SAAS
*   Late 1990

    Salesforce

*   SAAS

        Only allow pre-configured modules to support customers to run their business.

*   Early 2000

    Amazon.com

*   First time public cloud available


### Cloud computing deployment model


#### Private cloud

Organizations use their own hardware and software to deliver web services to their own internal users.



*   Privately shared virtualized resources
    *   Private enterprises (Government agencies) that has multiple databases and decided to move into cloud and host a single virtualized instance.
*   Cluster of dedicated customers
*   Connectivity via internet, dedicate private internet or fiber.
*   Good for secure confidential information system.


#### Public cloud

Offer computing services to all internet users: Example AWS, Microsoft Azure, google cloude



*   Publicly shared virtualized resources
*   Multiple customers
*   Connectivity via internet
*   Suites less for confidential information system


#### Hybrid cloud

Combination of both public and private cloud



*   Problem: Common authorization / federation


#### Community cloud

Community cloud pool computing resources and make them available to organisations with common needs.



*   Shared virtualized resources either managed by internally or 3rd party. 
*   Member organisation share a common goal and reduce costs.
*   Multitenancy

	Infrastructure shared between several organisations from same community. Each organisation has an isolated environment. 



*   Connectivity via internet and private network


### Organisations that benefits from cloud computing


#### slow time to market

The time it takes to bring a product from development to delivery. Cloud computing helps reduce this timeframe.


#### costly excess capacity

Company buy computing power for the peak hour usage and the resource will not be always utilized.


    We can use advantage of **Elasticity** by cloud providers.


#### rapid growth


    We can use advantage of **Elasticity** by cloud providers.


#### excess capital investment

Investment to setup the computing resources requirement. 



*   R&D
*   Time consuming
*   Data center

    We can use advantage of **resource pooling** by cloud providers.



### Organisations that might NOT benefits from cloud computing


#### Fixed predictable workloads



*   Workloads never change over the year


#### Existing large data centers



*   Virtualizations


#### Compliance issues



*   Some organization need physical access to data.


#### Operational assurances \
Health and safety organizations need real time operational service.


### Cloud service categories



*   SAAS - gmail/ salesforce

Software as a service (SaaS) is a model for the distribution of software where customers access software over the Internet typically with a web browser.



*   PAAS Platform as a service - g-suites/elastic beanstack 

    Platform as a service delivers key software and hardware components as a unit. An example of this would be a database server or web server.

*   IAAS - AWS/rackspace

Infrastructure as a service. This service model includes the creation of cloud servers, cloud-based storage, and backup.



*   XAAS anything as service

SLA?


## Cloud computing Business Perspectives


### Cloud computing vs outsourcing

Outsourcing is hiring a company to provide materials or service 



*   Defined by a service contract 
    *   Legal department
    *   (scope of the services and charges)
*   Example: Help desk, server maintenance

Why outsource?



*   Lower costs
*   Expertise
*   Unique game changing service

Cloud computing 



*   Form of outsourcing
*   Standard service offering
*   Multiple customers
*   Service contract via websites with card payment
*   Vendor lock-in 


### Scalability of cloud computing

Ability of application to meet changing demand



*   +/ - in number of users
*   + / - in number of transactions
*   + /-  in number of products

What does meant to business. 



*   IT department can shorten lead times to meet aggressive timelines
*   Company can respond quickly to unforeseen demand
*   New services can be integrated quickly
*   Can simplify capacity planning issues


### Security of cloud computing



*   Major security concerns
    *   Compliance ( HIPPA, PCI, FISMA … )
        *   Many company has regulation
    *   Confidentiality - security access groups 
        *   Data segregation 
    *   Data Integrity
        *   Data not been tempered by unauthorized party.
    *   Data loss
    *   Unauthorized access
*   How do cloud computing vendors address these?
    *   Compliance certificates
    *   Regular audits
    *   Robust backup and recovery system
    *   Increased logging and security reports
    *   Data delivery via HTTPS.


### Hardware independent of cloud computing

Hardware independence is the ability of an organization to have computer operations that are not dependent on a particular brand of hardware or computing vendor.



1.  Vendor lock-in - once you consume a service it's difficult to switch vendor. 
1.  Software compatibility
1.  Hardware upgrades
1.  Lead Time


### Variable costs of cloud computing



1.  Additional servers
1.  Server upgrades
1.  Storage upgrades
1.  Network upgrades

CAPEX expenses to OPEX.


### Time to market with cloud computing



*   Rapid innovation
*   Increased market share
*   Faster revenue growth
*   Superior customer loyalty


### Distribution of Cloud Computing Services over the Internet 



*   On-demand service and Broad network access
*   Data collection is easily centralized
*   Cloud data systems are very flexible (data collaboration)
*   Access to system administrators is simplified
*   Configure secure connection


## Technical Perspectives/Cloud Types


## Steps to Successful Adoption of Cloud Computing


## Impact and Changes of Cloud Computing on IT Service Management


## Risks and Consequences of Cloud Computing


# AWS Essentials


## Account Basics


### What is AWS Free Tier?

It refers to the limited free use of AWS services.



*   AWS offer free tier as means for a user to learn, experiment and get hands-on experience with AWS services.
*   Almost all AWS services offer some kind of Free Tier usage
*   Available for 12 months after you create AWS account
*   Some services extend Free Tier past 12 months
*   Free Tiers is only available to new AWS customers


### Using AWS Free Tier


### How to create AWS account?


### Navigate AWS Console



*   AWS service
*   Documentation
*   Support
*   Region selection
*   Account settings


## Identity & Access Management (IAM)


### What is IAM?

**How to: Introduction to Terraform Modules**


###### 
    **TABLE OF CONTENTS**



1.  WHAT ARE TERRAFORM MODULES?
1.  WHY USE TERRAFORM MODULES
1.  HOW TO INSTALL TERRAFORM IN 4 COMMANDS
1.  USING AN ADVANCED CODE EDITOR
1.  POSSIBLE MODULES REPOSITORIES
1.  HOW TO CREATE YOUR FIRST BASIC MODULE
    1.  THE MODULE CODE
    1.  THE MAIN TERRAFORM CODE
    1.  HOW TO USE GIT REPOS TO STORE THE MODULES
1.  TERRAFORM DESTROY
1.  CONCLUSION
1.  A MORE COMPLEX EXAMPLE


##### **WHAT ARE TERRAFORM MODULES?**

**Modules in the Terraform ecosystem are a way to organize the code to: **



*   **be more reusable**
*   **avoid code duplication**
*   **improve the code organization and its readability**

**If we compare them to a programming language, writing a Terraform module is the equivalent of writing a function and calling it many times with different parameters.**


##### **WHY USE TERRAFORM MODULES**

**By using modules, you will save time because you write the code once, test it and reuse it many times with different parameters. You can also use modules written by other people. In this git repo you can find some example community modules: [https://github.com/terraform-community-modules](https://github.com/terraform-community-modules)**

**When you want to change something you change it only in the module and all the infrastructure parts where the module is used will reflect the change when the next "terraform apply" will be run.**

**On the other side you need to be very careful when you change something or when you use a module maintained by another person and you want to update to a new version.**


##### **HOW TO INSTALL TERRAFORM IN 4 COMMANDS**

**Installing Terraform is pretty easy. You can find the download page of the latest version here: [https://www.terraform.io/downloads.html](https://www.terraform.io/downloads.html). At the moment the 0.78 version is available. In order to install it in Linux these 4 commands are enough:**


```
wget https://releases.hashicorp.com/terraform/0.7.8/terraform_0.7.8_linux_amd64.zip
unzip terraform_0.7.8_linux_amd64.zip
sudo mv terraform /usr/bin/
```


**Test if it is ok, with the command version you should obtain something like this:**


```
$ terraform version
Terraform v0.7.8
```



##### **USING AN ADVANCED CODE EDITOR**

**You can of course edit terraform code using Gedit or Vim or Notepad but the code will grow and having a syntax checker and color highlighter will surely simplify your life. My advice for a Terraform code editing tool is Atom.**

**You can download it for free from the main website [https://atom.io/](https://atom.io/) and you will also need to add the additional package for Terraform published on [https://github.com/GiantToast/language-terraform](https://github.com/GiantToast/language-terraform)**

**After installing Atom, in order to add the package in Linux it is necessary only to run the command**


```
apm install language-terraform
```


**For the purpose of this tutorial you can also use Vim or another editor because it is a proof of concept and fairly simple example.**


##### **POSSIBLE MODULES REPOSITORIES**

**A module is an independent piece of code and you can get it from many sources. There is a full list of possible sources on the official page [https://www.terraform.io/docs/modules/sources.html](https://www.terraform.io/docs/modules/sources.html), and you can get one of them and save your code from four kinds of repositories:**



1.  **Local File System**
1.  **AWS S3**
1.  **HTTP URLs**
1.  **Standard code repos mercurial/git, and there are also direct connections to BitBucket/GitHub**


##### **HOW TO CREATE YOUR FIRST BASIC MODULE**

**Let's create our first module to see this useful Terraform feature in action. We will use AWS as the provider. In the main Terraform code we will create a security group, call a module, and use the security group as a parameter. In the next paragraph we will see a more complex module but for our first steps in the Terraform module world this example module is complex enough.**

**We will do this twice using two kinds of repositories: "Local File System" and "Standard git repos".**


###### **THE MODULE CODE**

**For the module using the "Local File System" as the repo, we create a directory and place the code inside:**


```
cd ~
mkdir tf-module
cd tf-module
touch main-module.tf
touch variables.tf
```


**The main-module.tf code is:**


```
resource "aws_elb" "mybalancer" {
name = "${var.modname}"


 subnets         = ["${var.SubnetId}"]
 security_groups = ["${var.SecGroupId}"]


 listener {
instance_port     = 80
   instance_protocol = "http"
   lb_port           = 80
   lb_protocol       = "http"
 }


}
```


**The variables.tf code is (find a subnet id in your default vpc for the chosen region):**


```
variable modname {}
variable SubnetId {
default = "subnet-xxxxxxx"
 description = "The subnet id of a default vpc in the working region"
}
variable SecGroupId {
description = "this variable doesn't have a default value but it is passed from the main code"
}
```


**Run a Terraform validate to check for possible mistakes**


###### **THE MAIN TERRAFORM CODE**

**For our Terraform project let's create a directory and create the file resources.tf. Please remember that all the .tf files present in the directory (but not in the subdirectory) will be considered by the command line utility:**


```
cd ~
mkdir test-tf-project
cd test-tf-project
touch resources.tf
touch variables.tf
```


**Insert in the resource.tf this code:**


```
provider "aws" {
shared_credentials_file  = "/home/vagrant/.aws/credentials"
 region     = "${var.region}"
 profile    = "default"
}


resource "aws_security_group" "ForBalancer" {
name        = "For_Balancer"
 description = "A security group that will apply to the balancer"
 vpc_id      = "${var.myvpcid}"


# Allow HTTP from anywhere
 ingress {
from_port   = 80
   to_port     = 80
   protocol    = "tcp"
   cidr_blocks = ["0.0.0.0/0"]
 }
}


module "balancer1" {
source = "../tf-module"
 modname   = "Balancer1"
 SecGroupId = "${aws_security_group.ForBalancer.id}"
}
```


**Note that your shared_credentials_file may be in a different directory. Check ~/.aws/credentials on Linux, OS X, or Unix. Check C:\Users\USERNAME\.aws\credentials on Windows. We'll take a look at creating this file in a moment if you don't have one already.**

**Choose a region (I chose Mumbai) and use the default VPC ID for that region. Insert in the variables.tf this code:**


```
variable region {
default = "ap-south-1"
 description = "the aws region where we want create the resources"
}


variable myvpcid {
default = "vpc-xxxxxxx"
 description = "you can choose the default vpc of the chosen region"
}
```


**Run a Terraform validate to check for mistakes.**

**You also need to configure the Amazon credentials file and make sure you have the rights to creating resources in the Aws account:**


```
cat ~/.aws/credentials
[default]
aws_access_key_id = XXXXXXXXXX
aws_secret_access_key = YYYYYYYYYYYYYYYYYYYY
```


**If you don't have access key credentials, [here's how](https://aws.amazon.com/developers/access-keys/) you can create them.**

**If you check the directory at this point with an ls -a command you will notice that there aren't hidden directories and if you run the Terraform apply you will get an error like this:**


```
terraform apply

Error downloading modules: module balancer: not found, may need to be downloaded using 'terraform get'
```


**This happens the first time you use the module in this Terraform project because you haven't imported the module yet.**

**This is what happens when you run the Terraform get command for the first time:**


```
terraform get
Get: file:///home/vagrant/mod-example/tf-module


test-tf-project> ll -a
total 20K
drwxrwxrwx 3 vagrant vagrant 4.0K Nov 10 14:24 ./
drwxrwxrwx 4 vagrant vagrant 4.0K Nov 10 14:08 ../
-rwxrwxrwx 1 vagrant vagrant  216 Nov 10 14:18 resources.tf*
drwxrwxrwx 3 vagrant vagrant 4.0K Nov 10 14:24 .terraform/
-rwxrwxrwx 1 vagrant vagrant  116 Nov 10 14:11 variables.tf*


test-tf-project> cd .terraform/modules


test-tf-project/.terraform/modules> ls
05050e8862d9145225af71de66085340@


test-tf-project/.terraform/modules> ll
total 0
lrwxrwxrwx 1 vagrant vagrant 35 Nov 10 14:25 05050e8862d9145225af71de66085340 -> /home/vagrant/mod-example/tf-module/
```


**A new directory ".terraform/modules" is created and inside of it there will be a symlink to the module: this happens because we are using a module in the same file system. If you are running in a mounted file system where you don't have the rights to create symlinks you will get an error like this:**


```
Error loading Terraform: Error downloading modules: error downloading 'file:///vagrant/terraform/mod-example/tf-module': symlink /vagrant/terraform/mod-example/tf-module .terraform/modules/05050e8862d9145225af71de66085340: protocol error
```


**Now you can run the module and you will obtain something like:**


```
> terraform apply
aws_security_group.ForBalancer: Creating... description:                          "" => "A security group that will apply to the balancer"
…………………………………………………………………....
module.balancer1.aws_elb.mybalancer: Creation complete

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path: terraform.tfstate
```


**As you can see a new file was stored in the directory called terraform.tfstate. This file contains information regarding the state of the infrastructure you just created. If you do not change the files and the infrastructure in AWS and you run the Terraform apply command again you will see that nothing changes but a new file called terraform.tfstate.backup will appear. **


```
test-tf-project> terraform apply
aws_security_group.ForBalancer: Refreshing state... (ID: sg-6ee67896)
module.balancer1.aws_elb.mybalancer: Refreshing state... (ID: Balancer1)


Apply complete! Resources: 0 added, 0 changed, 0 destroyed.
```


**This happens because, just like CloudFormation, Terraform tries to modify the infrastructure for it to be equal to the one described by the .tf files and if it already matches it will not perform any change.**

**The last two copies of the tfstate file will be kept in the future runs as well.**

**If you check in your AWS console you can find two resources: the Security Group and the Elastic Load Balancer.**

**Now that we have a working module let's clarify some points:**



*   **You can pass parameters from the command line to the main Terraform code. In a similar way you can also pass values from the main Terraform code to the module**
*   **If you specify a default value in the variables file you can omit to pass a value when you name the module or the function, or you can choose to overwrite that default value**


###### **HOW TO USE GIT REPOS TO STORE THE MODULES**

**Let's test the same module using a git repos. For this purpose, I have used AWS CodeCommit but for Terraform there is no difference between CodeCommit and a standard git repo.**

**Commit the two module files (main-module.tf and variables.tf) in your personal git repo.**

**Setup a passwordless connection from your machine and your git repo. More info on using git  [https://linuxacademy.com/cp/modules/view/id/88](https://linuxacademy.com/cp/modules/view/id/88)**

**Modify the resources.tf this way:**


```
provider "aws" {
shared_credentials_file  = "/home/vagrant/.aws/credentials"
 region     = "${var.region}"
 profile    = "default"
}


resource "aws_security_group" "ForBalancer" {
name        = "For_Balancer"
 description = "A security group that will apply to the balancer"
 vpc_id      = "${var.myvpcid}"


# Allow HTTP from anywhere
 ingress {
from_port   = 80
   to_port     = 80
   protocol    = "tcp"
   cidr_blocks = ["0.0.0.0/0"]
 }
}


module "balancer1" {
source = "git::ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/test-module"
 modname   = "Balancer1"
 SecGroupId = "${aws_security_group.ForBalancer.id}"
}


module "balancer2" {
source = "git::ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/test-module"
 modname   = "Balancer2"
 SecGroupId = "${aws_security_group.ForBalancer.id}"
}
```


**Replace both "source" values with your own repository source.**

**I have declared the module twice and the only difference is the source line.**

**Now run the Terraform update, this time with the --update option. It is convenient to use this option because if in the future the module will be updated you can download the latest version and replace the one you have saved:**


```
test-terraform> terraform get --update
Get: git::ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/test-module (update)
Get: git::ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/test-module (update)
```


**If you look inside the .terraform/modules directory you will notice two subdirectories instead of one because the module was used twice and there is no symbolic link because the module source is not in the same file system.**


```
ls .terraform/modules/
2cc681f1005f9bfdec3c198e910ab2e5/  be314b5008b8dd431e1f85b19ce13ecb/
```


**Now by running the Terraform apply command three resources instead of two will be created:**


```
/test-terraform> terraform  apply
aws_security_group.ForBalancer: Creating…
…………………………………………………….
module.balancer2.aws_elb.mybalancer: Creation complete
module.balancer1.aws_elb.mybalancer: Creation complete


Apply complete! Resources: 3 added, 0 changed, 0 destroyed.
```



##### **TERRAFORM DESTROY**

**Don't forget to destroy all the resources at the end of the tutorial so the cost of this exercise will be close to zero dollars:**


```
test-tf-project> terraform destroy
Do you really want to destroy?
 Terraform will delete all your managed infrastructure.
 There is no undo. Only 'yes' will be accepted to confirm.


 Enter a value: yes


aws_security_group.ForBalancer: Refreshing state... (ID: sg-6eed6a07)
module.balancer1.aws_elb.mybalancer: Refreshing state... (ID: Balancer1)
aws_elb.mybalancer: Destroying...
aws_elb.mybalancer: Destruction complete
aws_security_group.ForBalancer: Destroying...
aws_security_group.ForBalancer: Still destroying... (10s elapsed)
aws_security_group.ForBalancer: Destruction complete


Apply complete! Resources: 0 added, 0 changed, 2 destroyed.
```



##### **CONCLUSION**

**As you can understand, if you extend the example with these module features you can create a complex module and use it many times with different parameters. The code will become shorter and simpler to manage compared to repeating the same code many times.**


##### **A MORE COMPLEX EXAMPLE**

**In this example you can see more complex code which uses a Terraform module.**

**Inside the module you have an AutoScaling Group, a Launch Configuration, an Elastic Load Balancer and a Route53 record definition and all these components assume different values every time you edit the module with different parameters.**

**This is the code which names the module from the main Terraform code part:**


```
module "BoxBackEnd" {
source = "git::ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/mybox"
 boxname = "Box1"
 vpcid = "${var.destination-vpc}"
 SubnetPrivate1 = "${aws_subnet.PrivateAZB.id}"
 SubnetPrivate2 = "${aws_subnet.PrivateAZC.id}" 
 AMI = "${var.AmiBox1BackEnd}"
 Machine_Key_Name = "${var.key_name}"
 Max_Size = "3"
 Min_Size = "2"
 ZoneID = "${aws_route53_zone.main.zone_id}"
 RecordName = "box1.mydomain.com"
 instance_port = "8080"
 instance_protocol = "tcp"
 load_balancer_port = "80"
 load_balancer_protocol = "tcp"
 Health_check_Target = "TCP:8080"
 inst_type = "t2.micro"
 secGroup-ELB = "${aws_security_group.box1-ELB.id}"
 secGroup-AUTOSCALE =  ${aws_security_group.box1-AUTOSCALE.id}"     
 userdata = <<HEREDOC
   #!/bin/bash
   docker restart logstash
HEREDOC
}
```


**This is the module code in variables.tf:**


```
variable boxname {}
variable vpcid {}
variable SubnetPrivate1 {}
variable SubnetPrivate2 {}
variable AMI {}
variable Machine_Key_Name {}
variable Max_Size {}
variable Min_Size {}
variable ZoneID {}
variable RecordName {}
variable instance_port {}
variable instance_protocol {}
variable load_balancer_port {}
variable load_balancer_protocol {}
variable Health_check_Target {}
variable inst_type {}
variable secGroup-ELB {}
variable secGroup-AUTOSCALE {}
variable userdata {}
```


**This is the module code you can put in any file with .tf extensions:**


```
resource "aws_elb" "ELB" {
name = "${var.boxname}${var.vpcid}"
 subnets = [ "${var.SubnetPrivate1}", "${var.SubnetPrivate2}"]
 security_groups = ["${var.secGroup-ELB}"]
 internal = true

 listener {
instance_port = "${var.instance_port}"
   instance_protocol = "${var.instance_protocol}"
   lb_port = "${var.load_balancer_port}"
   lb_protocol = "${var.load_balancer_protocol}"
 }

 health_check {
healthy_threshold = 2
   unhealthy_threshold = 2
   timeout = 3
   target = "${var.Health_check_Target}"
   interval = 30
 }
}

resource "aws_launch_configuration" "LaunchConfiguration" {
name = "${var.boxname}${var.vpcid}"
 image_id = "${var.AMI}"
 instance_type = "${var.inst_type}"
 security_groups = ["${var.secGroup-AUTOSCALE}"]
 key_name = "${var.Machine_Key_Name}"
 user_data = "${var.userdata}"
}

resource "aws_autoscaling_group" "AutoScaling" {
name =  "${var.boxname}${var.vpcid}"
 max_size = "${var.Max_Size}"
 min_size = "${var.Min_Size}"
 force_delete = true
 launch_configuration = "${aws_launch_configuration.LaunchConfiguration.name}"
 load_balancers = ["${aws_elb.ELB.name}"]
 vpc_zone_identifier = [ "${var.SubnetPrivate1}", "${var.SubnetPrivate2}"]
tag {
key = "Name"
   value = "${var.boxname}-AutoScaling"
   propagate_at_launch = "true"
 }
}

resource "aws_route53_record" "www" {
zone_id = "${var.ZoneID}"
  name = "${var.RecordName}"
  type = "CNAME"
  ttl = "300"
  records = ["${aws_elb.ELB.dns_name}"]
}
```


**Feel free to play with this more advanced example to get an understanding of how the different pieces work together, based on what we learned with our basic example.**


# A complete AWS environment with Terraform


###### 
    **TABLE OF CONTENTS**



1.  INTRODUCTION
    1.  PREREQUISITES
1.  THE FILES STRUCTURE
    1.  VARIABLES.TF
    1.  NETWORK.TF
    1.  ROUTING-AND-NETWORK.TF
    1.  SUBNETS.TF
    1.  DNS-AND-DHCP.TF
    1.  EC2-MACHINES.TF
1.  THE DATABASE MACHINE
1.  THE WEBAPP MACHINE
1.  RUNNING THE TERRAFORM AND CONNECT TO THE APPLICATION

###### 
    **RELATED GUIDES**

1.  [A COMPLETE AWS ENVIRONMENT WITH TERRAFORM](https://linuxacademy.com/cp/socialize/index/type/community_post/id/13922)
1.  [AUTOMATING TERRAFORM WITH JENKINS AND AWS CODECOMMIT](https://linuxacademy.com/cp/socialize/index/type/community_post/id/18753)


##### **INTRODUCTION**

The purpose of this article is to show a full AWS environment built using the Terraform automation. We will create everything you need from scratch: VPC, subnets, routes, security groups, an EC2 machine with MySQL installed inside a private network, and a webapp machine with Apache and its PHP module in a public subnet. The webapp machine reads a table in the database and shows the result.



<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS0.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS0.jpg "image_tooltip")



###### **PREREQUISITES**

There are only 2 prerequisites:



1.  Having Terraform installed: it is pretty easy to install it if you haven't already. You can find the instructions in my first article "[Introduction to Terraform Modules.](https://linuxacademy.com/cp/socialize/index/type/community_post/id/12369)"
1.  
1.  
1.  If you want to log in to the machines, you need to have an AWS pem key already created in the region of your choice and downloaded on your machine. See how to [create a pem key here](https://linuxacademy.com/cp/courses/lesson/course/535/lesson/3/module/66) if you haven't already.


##### **THE FILES STRUCTURE**

Terraform elaborates all the files inside the working directory so it does not matter if everything is contained in a single file or divided into many, although it is convenient to organize the resources in logical groups and split them into different files. Let's take a look at how we can do this effectively:


###### **VARIABLES.TF**


```
variable "region" {
  default = "us-west-2"
}
variable "AmiLinux" {
  type = "map"
  default = {
    us-east-1 = "ami-b73b63a0"
    us-west-2 = "ami-5ec1673e"
    eu-west-1 = "ami-9398d3e0"
  }
  description = "I add only 3 regions (Virginia, Oregon, Ireland) to show the map feature but you can add all the r"
}
variable "aws_access_key" {
  default = ""
  description = "the user aws access key"
}
variable "aws_secret_key" {
  default = ""
  description = "the user aws secret key"
}
variable "vpc-fullcidr" {
    default = "172.28.0.0/16"
  description = "the vpc cdir"
}
variable "Subnet-Public-AzA-CIDR" {
  default = "172.28.0.0/24"
  description = "the cidr of the subnet"
}
variable "Subnet-Private-AzA-CIDR" {
  default = "172.28.3.0/24"
  description = "the cidr of the subnet"
}
variable "key_name" {
  default = ""
  description = "the ssh key to use in the EC2 machines"
}
variable "DnsZoneName" {
  default = "linuxacademy.internal"
  description = "the internal dns name"
}
```


All variables are defined in the variables.tf file. Before you run the the "terraform apply" command, you need to insert your access and secret keys. If you also want to log into the EC2 machine, make sure you fill in the key name as well.

Every variable is of type String, except for the AmiLinux. This particular variable is a map and depends on the content of the region variable. You can add the region you wish to use in the map using the ami-id of the AWS Linux distribution.


###### **NETWORK.TF**


```
provider "aws" {
  access_key = "${var.aws_access_key}"
  secret_key = "${var.aws_secret_key}"
  region     = "${var.region}"
}
resource "aws_vpc" "terraformmain" {
    cidr_block = "${var.vpc-fullcidr}"
   #### this 2 true values are for use the internal vpc dns resolution
    enable_dns_support = true
    enable_dns_hostnames = true
    tags {
      Name = "My terraform vpc"
    }
}
```


In the network.tf file, we set up the provider for AWS and the VPC declaration. Together with the Route53 configuration, the option specified for the vpc creation enables an internal name resolution for our VPC. As you may be aware, Terraform can be used to build infrastructures for many environments, such as AWS, Azure, Google Cloud, VMware, and many others. A full list is available here:  [https://www.terraform.io/docs/providers/index.html](https://www.terraform.io/docs/providers/index.html) . In this article, we are using AWS as the provider.


###### **ROUTING-AND-NETWORK.TF**


```
# Declare the data source
data "aws_availability_zones" "available" {}

/* EXTERNAL NETWORG , IG, ROUTE TABLE */
resource "aws_internet_gateway" "gw" {
   vpc_id = "${aws_vpc.terraformmain.id}"
    tags {
        Name = "internet gw terraform generated"
    }
}
resource "aws_network_acl" "all" {
   vpc_id = "${aws_vpc.terraformmain.id}"
    egress {
        protocol = "-1"
        rule_no = 2
        action = "allow"
        cidr_block =  "0.0.0.0/0"
        from_port = 0
        to_port = 0
    }
    ingress {
        protocol = "-1"
        rule_no = 1
        action = "allow"
        cidr_block =  "0.0.0.0/0"
        from_port = 0
        to_port = 0
    }
    tags {
        Name = "open acl"
    }
}
resource "aws_route_table" "public" {
  vpc_id = "${aws_vpc.terraformmain.id}"
  tags {
      Name = "Public"
  }
  route {
        cidr_block = "0.0.0.0/0"
        gateway_id = "${aws_internet_gateway.gw.id}"
    }
}
resource "aws_route_table" "private" {
  vpc_id = "${aws_vpc.terraformmain.id}"
  tags {
      Name = "Private"
  }
  route {
        cidr_block = "0.0.0.0/0"
        nat_gateway_id = "${aws_nat_gateway.PublicAZA.id}"
  }
}
resource "aws_eip" "forNat" {
    vpc      = true
}
resource "aws_nat_gateway" "PublicAZA" {
    allocation_id = "${aws_eip.forNat.id}"
    subnet_id = "${aws_subnet.PublicAZA.id}"
    depends_on = ["aws_internet_gateway.gw"]
}
```


When you start from scratch, you need to attach an internet gateway to your VPC and define a network ACL. There aren't restriction at network ACL level because the restriction rules will be enforced by security group.

As you can see, there are two routing tables: one for public access, and the other one for private access. In our case, we also need to have access to the internet from the database machine since we use it to install MySQL Server. We will use the AWS NAT Gateway in order to increase our security and be sure that there aren't incoming connections coming from outside the database. As you can see, defining a NAT gateway is pretty easy since it consists of only four lines of code. It is important, though, to deploy it in a public subnet and associate an elastic ip to it. The depends_on allows us to avoid errors and create the NAT gateway only after the internet gateway is in the available state.

One thing worth noting is that the data called aws_availability_zones provide the correct name of the availability zones in the chosen region. This way we don't need to add letters to the region variable and we can avoid mistakes. For example, the North Virginia region where region b does not exist, and in other regions where there are 2 or 4 AZs .


###### **SUBNETS.TF**


```
resource "aws_subnet" "PublicAZA" {
  vpc_id = "${aws_vpc.terraformmain.id}"
  cidr_block = "${var.Subnet-Public-AzA-CIDR}"
  tags {
        Name = "PublicAZA"
  }
 availability_zone = "${data.aws_availability_zones.available.names[0]}"
}
resource "aws_route_table_association" "PublicAZA" {
    subnet_id = "${aws_subnet.PublicAZA.id}"
    route_table_id = "${aws_route_table.public.id}"
}
resource "aws_subnet" "PrivateAZA" {
  vpc_id = "${aws_vpc.terraformmain.id}"
  cidr_block = "${var.Subnet-Private-AzA-CIDR}"
  tags {
        Name = "PublicAZB"
  }
  availability_zone = "${data.aws_availability_zones.available.names[1]}"
}
resource "aws_route_table_association" "PrivateAZA" {
    subnet_id = "${aws_subnet.PrivateAZA.id}"
    route_table_id = "${aws_route_table.private.id}"
}
```


There are two subnets associated with the respective routes: a public and a private.


###### **DNS-AND-DHCP.TF**


```
resource "aws_vpc_dhcp_options" "mydhcp" {
    domain_name = "${var.DnsZoneName}"
    domain_name_servers = ["AmazonProvidedDNS"]
    tags {
      Name = "My internal name"
    }
}

resource "aws_vpc_dhcp_options_association" "dns_resolver" {
    vpc_id = "${aws_vpc.terraformmain.id}"
    dhcp_options_id = "${aws_vpc_dhcp_options.mydhcp.id}"
}

/* DNS PART ZONE AND RECORDS */
resource "aws_route53_zone" "main" {
  name = "${var.DnsZoneName}"
  vpc_id = "${aws_vpc.terraformmain.id}"
  comment = "Managed by terraform"
}

resource "aws_route53_record" "database" {
   zone_id = "${aws_route53_zone.main.zone_id}"
   name = "mydatabase.${var.DnsZoneName}"
   type = "A"
   ttl = "300"
   records = ["${aws_instance.database.private_ip}"]
}
```


In this file, three things were accomplished: the private Route53 DNS zone was created, the association with the VPC was made, and the DNS record for the database was created. Terraform perform the actions in the right order, the last component in this file will be the database dns record because it depends on the private ip of the EC2 database machine. This machine will be allocated during the database creation.



<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS1.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS1.jpg "image_tooltip")


securitygroups.tf


```
resource "aws_security_group" "FrontEnd" {
  name = "FrontEnd"
  tags {
        Name = "FrontEnd"
  }
  description = "ONLY HTTP CONNECTION INBOUD"
  vpc_id = "${aws_vpc.terraformmain.id}"

  ingress {
        from_port = 80
        to_port = 80
        protocol = "TCP"
        cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    from_port   = "22"
    to_port     = "22"
    protocol    = "TCP"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "Database" {
  name = "Database"
  tags {
        Name = "Database"
  }
  description = "ONLY tcp CONNECTION INBOUND"
  vpc_id = "${aws_vpc.terraformmain.id}"
  ingress {
      from_port = 3306
      to_port = 3306
      protocol = "TCP"
      security_groups = ["${aws_security_group.FrontEnd.id}"]
  }
  ingress {
      from_port   = "22"
      to_port     = "22"
      protocol    = "TCP"
      cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```


We have two security groups: one for the web application, and another for the database. They both need to have the outbound (egress) rule to have internet access because yum will install the Apache and MySQL servers, but the connection to the MySQL port will be allowed only from instances that belong to the webapp security group.

I have left the ssh port open only for debug reason, but you can also delete that rule.


###### **EC2-MACHINES.TF**


```
resource "aws_instance" "phpapp" {
  ami           = "${lookup(var.AmiLinux, var.region)}"
  instance_type = "t2.micro"
  associate_public_ip_address = "true"
  subnet_id = "${aws_subnet.PublicAZA.id}"
  vpc_security_group_ids = ["${aws_security_group.FrontEnd.id}"]
  key_name = "${var.key_name}"
  tags {
        Name = "phpapp"
  }
  user_data = <<HEREDOC
  #!/bin/bash
  yum update -y
  yum install -y httpd24 php56 php56-mysqlnd
  service httpd start
  chkconfig httpd on
  echo "<?php" >> /var/www/html/calldb.php
  echo "\$conn = new mysqli('mydatabase.linuxacademy.internal', 'root', 'secret', 'test');" >> /var/www/html/calldb.php
  echo "\$sql = 'SELECT * FROM mytable'; " >> /var/www/html/calldb.php
  echo "\$result = \$conn->query(\$sql); " >>  /var/www/html/calldb.php
  echo "while(\$row = \$result->fetch_assoc()) { echo 'the value is: ' . \$row['mycol'] ;} " >> /var/www/html/calldb.php
  echo "\$conn->close(); " >> /var/www/html/calldb.php
  echo "?>" >> /var/www/html/calldb.php
HEREDOC
}

resource "aws_instance" "database" {
  ami           = "${lookup(var.AmiLinux, var.region)}"
  instance_type = "t2.micro"
  associate_public_ip_address = "false"
  subnet_id = "${aws_subnet.PrivateAZA.id}"
  vpc_security_group_ids = ["${aws_security_group.Database.id}"]
  key_name = "${var.key_name}"
  tags {
        Name = "database"
  }
  user_data = <<HEREDOC
  #!/bin/bash
  yum update -y
  yum install -y mysql55-server
  service mysqld start
  /usr/bin/mysqladmin -u root password 'secret'
  mysql -u root -psecret -e "create user 'root'@'%' identified by 'secret';" mysql
  mysql -u root -psecret -e 'CREATE TABLE mytable (mycol varchar(255));' test
  mysql -u root -psecret -e "INSERT INTO mytable (mycol) values ('linuxacademythebest') ;" test
HEREDOC
}
```


We chose an AWS Linux AMI. I loaded the userdata using the HEREDOC option, but you can also use an external file.


##### **THE DATABASE MACHINE**

This machine is placed in the private subnet and has its security group. The userdata performs the following actions:



*   update the OS
*   install the MySQL server and run it
*   configure the root user to grant access from other machines
*   create a table in the test database and add one line inside


##### **THE WEBAPP MACHINE**

It is placed in the public subnet so it is possible to reach it from your browser using port 80. The userdata performs the following actions:



*   update the OS
*   
*   install the Apache web server and its php module
*   
*   start the Apache
*   
*   using the echo command place in the public directory, a php file that reads the value inside the database created in the other EC2
*   


##### **RUNNING THE TERRAFORM AND CONNECT TO THE APPLICATION**

Create all files with extension .tf inside a directory, replace the values in the variable.tf as explained in the first part of the article, and then run the command:


```
terraform apply
```


After a few minutes, the process should be completed and you can go to your AWS web console and read the public ip of your EC2 machine. Visit the url in your browser, and you will see the result of the php command.



<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS2.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS2.jpg "image_tooltip")


Testing the zone

To test your internal DNS routing system, you can log in inside the web server machine to run a DNS query for the private zone like this:


```
$ host mydatabase.linuxacademy.internal
mydatabase.linuxacademy.internal has address 172.28.3.142
```


If you try to do it from a machine outside the vpc, you will have:


```
host mydatabase.linuxacademy.internal.
Host mydatabase.linuxacademy.internal. not found: 3(NXDOMAIN)
```


[YouTube](https://youtu.be/GjFVIDqDjA4) Tutorial and [github](https://github.com/giuseppeborgese/a-complete-aws-environment-with-terraform) repo


# Using AWS RDS with Laravel - October CMS


###### **AMAZON RDS**

**What is Amazon RDS?**

**Amazon Web Service offers a relational database service known as RDS. RDS allows you to use MySQL, PostgreSQL, MariaDB, Oracle, Microsoft SQL server and Amazon Aurora. RDS offers backups, recovery, software updates / patching and automatic failure detection.**

**Why choose Amazon RDS over a self-hosted solution?**

**Configuring and managing a database or cluster of databases can be a large pain point of an application / service. When you manage your own database, there are many things you need to consider: hardware, backups / recovery, security, software updates, performance, availability and more. Amazon RDS offers a solution to many of these points. You can easily scale your database down or up depending on traffic, storage requirements, or performance needs. RDS comes with security options but also allows you to control security through AWS IAM to handle users and permissions. **

**Disadvantages of Amazon RDS**

**RDS does not provide shell access to the RDS instances. RDS also restricts access to some system procedures and tables that need advanced privileges. **

**Getting started with Amazon RDS**

**We are going to set up a simple RDS instance which will be part of the AWS free tier. To get started, you need to log into the AWS dashboard and click on the ' Services' drop down menu. Find the ' Database' section and click on the ' RDS' item: **



<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS3.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS3.png "image_tooltip")


**You will now be within the RDS dashboard. Click on the ' Instances' link. This is the instances dashboard which will allow you to see any running RDS instances. They should be empty, so let' s click on the "Launch DB Instance" button to get started.**



<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS4.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS4.png "image_tooltip")


**When creating our instance, the first configuration option is choosing a database engine. To learn more about each engine and the support AWS offers (such as maximum memory), click on the tab. For this guide, we will be selecting MySQL. **

**dashboard which will allow you to see any running RDS instances. They should be empty, so let' s click on the "Launch DB Instance" button to get started. When creating our instance, the first configuration option is choosing a database engine. To learn more about each engine and the support AWS offers (such as maximum memory), click on the tab. For this guide, we will be selecting MySQL. **

<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS5.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS5.png "image_tooltip")


**We can see that it' s ' free tier eligible' and supports database sizes up to 6 TB. Click on the blue ' Select' button to continue. Next, we will be asked to select our environment type - production or development. **



<p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS6.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS6.jpg "image_tooltip")


**To help keep this guide straight forward, we will be using the ' Dev/Test' environment type as this also part of the AWS free tier.**

**We now need to specify the database details. **

**Instance Specifications**

**We' ll be creating a free tier RDS instance so make sure to tick the ' Only show options that are eligible for RDS Free Tier' option. The form will then update to match that option.**



<p id="gdcalert8" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS7.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert9">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS7.png "image_tooltip")


**Unless your application has any specific requirements, such as the MySQL DB engine version or the hardware (DB Instance class), then you can change them here before continuing.**

**Database Settings**

**Here is where we will set up our database credentials. We need to set a ' DB Instance Identifier' to easily identify this instance, a master username and master password.**



<p id="gdcalert9" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS8.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert10">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS8.png "image_tooltip")


**_Make a note of your master username and password as you will require this later. _**

**Please note: to keep this guide simple, we will not look into advanced security. We will only be using the bare minimum credentials to get us started and connected to our RDS instance. I recommend that you take some time to configure your security settings. You can learn more here: [http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.html](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.html)**

**For the network and security, this will require you to choose a few things that will be different to my setup. For this case, I will be using the Default VPC, default subnet group, make the instance publicly accessible (this is required for us to access the instance from our home machine), and select a VPC security group. For this instance, I will be creating a new security group that will add a rule to allow our IP address access into the instance.**



<p id="gdcalert10" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS9.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert11">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS9.png "image_tooltip")


**Our next step is to configure our database. We need to select a database name, a database port and to keep things straightforward. We will use the default parameter and option group. We do not need to enable the ' copy tags to snapshots' option or the encryption option unless your application requires it. Make a note of your database name as we will need it later.**



<p id="gdcalert11" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS10.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert12">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS10.png "image_tooltip")


**Backing up with RDS**

**RDS instances currently only support automatic backups if you are using the InnoDB storage engine. In this section, you can select the ' backup retention period' , which is how long AWS will save your instances backups. This will depend on your application' s requirements, but I will be setting it to 7 days for now. The backup window is used by AWS to help schedule the instances backup. A this may take up more resources, it can be worth setting this to a window when your application has the least users.**



<p id="gdcalert12" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS11.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert13">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS11.png "image_tooltip")


**Within the monitoring section, there is a single option - ' enable enhanced monitoring' . EEM allows you to get access to over 50 metrics about your RDS instance, including CPU, memory, and file system. It works on all instances types except t1.micro and m1.small and requires MySQL version 5.6+. **

**Within the monitoring section, we can control whether our RDS instances gets upgraded by minor versions automatically or not. This can be good for security as keeping MySQL update will patch any known exploits or security holes. The maintenance window is used by AWS to install any patches or handle any maintenance issues. **

**Once configured, we can launch the RDS instance. Press the blue "Launch DB Instance" to continue. AWS will now not begin launching the instance: **



<p id="gdcalert13" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS12.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert14">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS12.png "image_tooltip")


**Click on the "Instances" item on the left side menu and our new instance should appear in the list (with the status of ' creating' ):**



<p id="gdcalert14" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS13.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert15">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS13.png "image_tooltip")


**This can take 5 - 10 minutes to configure and back up but the status will change once complete and you will see the CPU usage and current activity:**



<p id="gdcalert15" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS14.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert16">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS14.png "image_tooltip")


**When you connect later in this guide, the current activity will update in real time:**



<p id="gdcalert16" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS15.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert17">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS15.png "image_tooltip")


**We now need to get the hostname and port number to continue onto the next section. You can get this information by simply clicking on the downwards arrow (˅) next to your instance. You should then see an "Endpoint" value which is your instance's hostname, an example would be:**


```
test.ign3o5in3go.eu-west-2.rds.amazonaws.com:3306
```


**Let' s now install October CMS and connect to our RDS instance. **


###### **LARAVEL APPLICATION - OCTOBER CMS**

**Our Laravel application of choice will be October CMS. October is a content management**

**system powered by Laravel and uses MySQL as the database. This will be a good test of Amazon' s RDS service since October uses many tables and schemas to handle the different aspects of the system, such as admin management, page management, and system customization. **

**How to install October CMS**

**Prerequisites**

**Before you get started, you must have the following installed on your system: a minimum version of PHP 5.5.9, the PHP mbstring extension, ext-dom, ext-curl, mysql/pdo, unzip, and have access to composer. **

**To make development and deployments easier, we will use composer (todo - add link) to handle our dependencies for October CMS and Laravel. To create the project scaffolding, we can run the following command:**


```
composer create-project october/october rds-demo
```


**We then need to update composer and install any project dependencies. Make sure you have moved into the folder by running the following command:**


```
cd rds-demo
```


**then**


```
composer update
```


**This make take a few minutes depending on your internet connection. Once composer has been installed and updated, we will have access to a new set of artisan commands. We now need to finish off the October CMS and update it:**


```
php artisan october:update
```


**October CMS is now installed and up to date. You can run the following command to get it up and running: **


```
php artisan serve
```


**Artisan will now start serving the software on port 8000. Visit this address in your browser and you should see the start / demo page:**



<p id="gdcalert17" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS16.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert18">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS16.png "image_tooltip")


**We now need to finish off the installation by configuring the database. Run the following command:**


```
php artisan october:install
```


**Select your database type. Let' s select 0 (which should be MySQL). We now need your credentials from earlier - database name, master username, master password and hostname.**

**Set the host as your instance' s host address and continue by pressing enter. We can now choose the port for MySQL, if you did not change this when setting up your instance then we can use the default port, which is 3306. **

**The next steps: **



*   **Database name**
*   **MySQL login**
*   **MySQL password**

**Now we set up a user for October CMS:**



*   **First Name**
*   **Last Name**
*   **Email Address**
*   **Admin Login**
*   **Admin Password**
*   **Information correct?**
*   
*   **Application URL**
*   **Configure advanced options?**
*   


###### **DATABASE**

**We now need to run our database migrations that will allow October CMS to add in tables and data into our RDS database instance. You can do this by running the following command:**


```
php artisan october:up
```


**If this command was successful, then your RDS instance will have 24 tables available. Let' s log in to the instance and check for them. **

**Logging into RDS via the command line**

**Since we have set up a MySQL powered database on RDS, we can interact with it using the mysql command line tool in Linux (you must have MySQL installed to access this). You can run the following command to get connected:**


```
mysql -h [HOST] -u [USERNAME] -p
```


**(We need to use the username that we added when we configured the RDS instance. The host will be available once the RDS has fully complete setting up and is live. You can find the host by accessing the RDS dashboard and going into the instances dashboard.) **

**Once you press enter, you will be prompted to enter your password. Type it, then press enter again. If your credentials are correct, then you should now be within the MySQL monitor. **

**Interacting with RDS data**

**To list the databases available, run the following command:**


```
show databases;
```


**Here, we can see the ' octobercms_dev_db' database that I set up during the RDS configuration: **



<p id="gdcalert18" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS17.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert19">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS17.png "image_tooltip")


**Let' s access that table to view the data. Run the following commands:**

** **


```
use octobercms_dev_db;
show tables;
```


**You should now see the tables that October CMS added in. Here is a snippet:**



<p id="gdcalert19" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS18.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert20">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS18.png "image_tooltip")


**Let' s view backend / admin users. We can do this by running the following command:**


```
SELECT * FROM backend_users;
```


**Depending on your initial configuration (when running the php artisan october:install command), you should see 1 user within the ' backend_users' table. This is the default admin added in by October CMS.**

**Let' s log in to October CMS using the default admin account. Visit the following url to access the admin backend: **


```
http://localhost/backend/
```


**Depending on your setup, this may be another IP address a different port, such as localhost:8000. **

**Click on the ' settings' menu from the top, and then click the ' Administrators' item from the left hand side menu. At the top of the page, click the ' New Administrator' button and fill out the form. When you are happy with the settings, click the ' create' button to add a new administrator. If you go back into the MySQL cli tool and run the same command again as above, you should now see two results returned: the admin account and the new one that you just added. **


###### **RESOURCES**



*   **[Laravel website](https://laravel.com/)**
*   **[October CMS website](http://octobercms.com/)**
*   **[October CMS installation guide](https://octobercms.com/docs/console/commands#console-install-composer)**
*   **[AWS Relational Database Service](https://aws.amazon.com/rds/)**
*   **[AWS RDS Security](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.html)**
*   

<p style="text-align: right">
<strong> 55  </strong></p>


**[...Show Previous Comments](https://linuxacademy.com/cp/socialize/index/type/community_post/id/15653#)**



<p id="gdcalert20" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS19.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert21">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS19.jpg "image_tooltip")


** **

**[Oliver Russell](https://linuxacademy.com/cp/socialize/profile/user/olidev)8/7/2018**


    **How is AWS RDS is different from the regular database? I am [hosting laravel project](https://www.cloudways.com/en/laravel-hosting.php) on AWS with a managed platform and regular MySQL/MariaDB database. Why should I go with RDS? **

<p style="text-align: right">
<strong> 0   </strong></p>




<p id="gdcalert21" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/AWS20.jpg). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert22">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/AWS20.jpg "image_tooltip")


** **

**[Daniel Kelley](https://linuxacademy.com/cp/socialize/profile/user/dkelley3742)9/14/2018**


    **Oliver,  **


    **The author answers your question in the second paragraph.  RDS is a managed database service so Amazon's IT department takes care of the database management stuff.  If you are already managing your own database then you probably are managing the instance, backups & recovery, security, software patches ... etc. So RDS manages this database stuff for you.**


```
    Why choose Amazon RDS over a self-hosted solution?
    Configuring and managing a database or cluster of databases can be a large pain point of an application / service. When you manage your own database, there are many things you need to consider: hardware, backups / recovery, security, software updates, performance, availability and more. Amazon RDS offers a solution to many of these points. You can easily scale your database down or up depending on traffic, storage requirements, or performance needs. RDS comes with security options but also allows you to control security through AWS IAM to handle users and permissions.
```



    **I  like to use the diagram in the "Who manages Cloud IAAS, PAAS,  and SAAS services?" article to help keep the difference straight in my brain.**


    **[https://mycloudblog7.wordpress.com/2013/06/19/who-manages-cloud-iaas-paas-and-saas-services/](https://mycloudblog7.wordpress.com/2013/06/19/who-manages-cloud-iaas-paas-and-saas-services/)**


<!-- Docs to Markdown version 1.0β14 -->
